""" Define the network architecture.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.rnn as g
import numpy as np

import sys
print(sys.path)
sys.path.append("src")
sys.path.append("src/models")
import encoders
import decoders

gpu = torch.cuda.is_available()


class Seq2Seq(nn.Module):
    """
    Sequence-to-sequence model at high-level view. It is made up of an EncoderRNN module and a DecoderRNN module.
    """

    def __init__(self, input_size, target_size, hidden_size, 
                 encoder_layers, decoder_layers,
                 output_type, drop_p=0.):
        """
        Args:
            target_size (integer): Target vocabulary size.
            hidden_size (integer): Size of GRU cells.
            encoder_layers (integer): EncoderRNN layers.
            decoder_layers (integer): DecoderRNN layers.
            drop_p (float): Probability to drop elements at Dropout layers.
        """
        super(Seq2Seq, self).__init__()

        self.encoder = encoders.EncoderRNN(input_size, hidden_size, encoder_layers, drop_p)
        if output_type == "phones":
            self.decoder = decoders.PhoneDecoderRNN(
                target_size, hidden_size, decoder_layers, drop_p)
        elif output_type == "bin":
            self.decoder = decoders.BinFeatDecoderRNN(
                target_size, hidden_size, decoder_layers, drop_p) 
        elif output_type == "cont":
            self.decoder = decoders.ContFeatDecoderRNN(
                target_size, hidden_size, decoder_layers, drop_p)
        else:
            raise NotImplementedError
            
    def forward(self, xs, xlens, ys=None):
        """
        The forwarding behavior depends on if ground-truths are provided.
        Args:
            xs (torch.LongTensor, [batch_size, seq_length, dim_features]): A mini-batch of FBANK features.
            xlens (torch.LongTensor, [batch_size]): Sequence lengths before padding.
            ys (torch.LongTensor, [batch_size, padded_length_of_target_sentences]): Padded ground-truths.
        Returns:
            * When ground-truths are provided, it returns cross-entropy loss. Otherwise it returns predicted word IDs
            and the attention weights.
            loss (float): The cross-entropy loss to maximizing the probability of generating ground-truth.
            predictions (torch.FloatTensor, [batch_size, max_length]): The sentence generated by Greedy Search.
            attn_weights (torch.FloatTensor, [batch_size, max_length, length_of_encoder_states]): A list contains
                attention alignment weights for the predictions.
        """
        if ys is None:
            predictions, attn_weights = self.decoder(self.encoder(xs, xlens))
            return predictions, attn_weights
        else:
            loss = self.decoder(self.encoder(xs, xlens), ys)
            return loss
